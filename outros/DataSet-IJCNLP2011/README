********************************************************************************
*
*    Copyright (C) 2011 by
*
*      Yaliang LI, email: ylli@smu.edu.sg
*      Jing JIANG, email: jingjiang@smu.edu.sg
*
*      School of Information Systems
*      Singapore Management University
*
*******************************************************************************


-------------------
General Information
-------------------
This directory contains the data sets we used in the following paper:
Yaliang Li, Jing Jiang, Hai Leong Chieu, and Kian Ming A. Chai. Extracting Relation descriptors with Conditional Random Fields. In proceedings of the 5th International Joint Conference on Nature Language Processing, 2011.


--------------------
Directory structure
--------------------

    DataSet-IJCNLP2011
    |-- data sets
    |   |-- New York Times.txt          # New York Times Data Set
    |   |-- Wikipedia.txt               # Wikipedia Data Set
    |-- utility
    |   |-- general-POS-tag.txt         # general POS tags we used
    |-- README                          # this file


---------------------------------------------------
Data Set Description -- generate candidate instance
---------------------------------------------------

New York Times data set contains 150 business articles from New York Times. The articles were crawled from the NYT website between November 2009 and January 2010. After sentence splitting and tokenization, we used the Stanford NER tagger (URL: http://nlp.stanford.edu/ner/index.shtml) to identify PER and ORG named entities from each sentence. For named entities that contain multiple tokens we concatenated them into a single token. We then took each pair of (PER, ORG) entities that occur in the same sentence as a single candidate relation instance, where the PER entity is treated as ARG-1 and the ORG entity is treated as ARG-2. 

Wikipedia data set comes from (URL: http://www.cs.umass.edu/~culotta/data/wikipedia.html), previously created by Aron Culotta et al.. Since the original data set did not contain the annotation information we need, we re-annotated it. Similarly, we performed sentence splitting, tokenization and NER tagging, and took pairs of (PER, PER) entities occurring in the same sentence as a candidate relation instance. We always treat the first PER entity as ARG-1 and the second PER entity as ARG-2.

A human annotator manually went through each candidate relation instance to decide (1) whether there is a relation between the two arguments and (2) whether there is an explicit sequence of words describing the relation held by ARG-1 and ARG-2.


-------------------------------------------
Data Set Description -- annotation criteria
-------------------------------------------

We only consider explicitly stated relation descriptors. If we cannot find such a relation descriptor, even if ARG-1 and ARG-2 actually have some kind of relation, we still label the instance as Nil. For example, in the instance "he is the son of ARG1 and ARG2", although we can infer that ARG-1 and ARG-2 have some family relation, we regard this as a negative instance.

A relation descriptor may also contain multiple relations. For example, in the instance "ARG1 is the CEO and president of ARG2", we label "the CEO and president" as the relation descriptor, which actually contains two job titles, namely, CEO and president.

Note that our annotated relation descriptors are not always nouns or noun phrases. An example can be "ARG1 later married ARG2 in 1973", where the relation descriptor "married" is a verb and indicates a spouse relation.


---------------------------------------
Data Set Description -- instance format 
---------------------------------------
For each instance, there is one line of the following format:
instance-ID    positive-label(1)/negative-label(0)
Then for each token in this instance4, there is one line of the following format:
token-ID    token    general-POS-tag    phrase-boundary-information    descriptor-label
Instead of the standard Penn Treebank POS tag set, we use a smaller set of general POS tags. Please refer the file utility/general-POS-tag.txt for more information.
To get the phrase boundary information, we use Illinois Chunker (URL: http://cogcomp.cs.illinois.edu/page/software_view/13).
Following the commonly used BIO notation in sequence labeling, we define the descriptor label set as {B-R, I-R, O}, where B-R is the beginning of the relation descriptor, I-R is inside of the relation descriptor, and O is outside of the relation descriptor.

Here is an example of positive instance:
NYT-24	1
0	said	V	B-VP	O
1	arg1	N	B-NP	O
2	,	,	O	O
3	the	DET	B-NP	B-R
4	governor	N	I-NP	I-R
5	of	PREP	B-PP	O
6	the	DET	B-NP	O
7	arg2	N	I-NP	O
8	,	,	O	O
9	in	PREP	B-PP	O
10	a	DET	B-NP	O
11	statement	N	I-NP	O
12	accompanying	V	B-VP	O
13	the	DET	B-NP	O
14	rate	N	I-NP	O
15	announcement	N	I-NP	O
16	tuesday	N	B-NP	O
17	.	.	O	O

--------------------------------- END --------------------------------
